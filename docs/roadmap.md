# 実装ロードマップ（現実ベース）

このプロジェクトで一番の敵は「モデル」じゃなく **端末/ブラウザの音声周りの仕様差**。  
ここを舐めると時間が溶ける。

## Phase 0: Scaffold（完了）
- Repo雛形
- Worker + Assets + D1 + R2 + Queue + DO の結線（placeholder）
- 子どもUI/親UI（placeholder）

## Phase 1: “娘が遊べる”最小体験（1〜2日）
- 子どもUIで「押す→話す→返事」が成立
- 返事は当面テキストのみ（音声は後）

成功条件:
- iPhoneで5分遊んで破綻しない（クラッシュ/固まり/操作不能が無い）

## Phase 2: Raw audio を確実に保存（最重要）
- MediaRecorder で録音 → R2 に保存
- events に audio key を保存
- 親画面で「音声が存在する」ことが確認できる

成功条件:
- 1日分の会話がR2に溜まり続ける

## Phase 3: ASR の二重化
- 可能なら Web Speech（即時） + raw audio（保存）
- Web Speech が無い/不安定なら、サーバASR（Whisper等）に切替可能にする
- 再解析ジョブ（Queue）を追加

成功条件:
- “認識がコケても記録は残る”状態

## Phase 4: LLM で返事（安全プロンプト付き）
- LLM provider abstraction（Anthropic/OpenAI/Cloudflare AIなど）
- 3歳向けの応答制約（短い・やさしい・危険回避）
- 口調の固定（“私のウサギ”）

成功条件:
- 返事が一貫していて、子どもが離脱しない

## Phase 5: TTS キャッシュ（体験が急に良くなる）
- 返事テキスト → TTS生成 → R2保存 → 次回同一文は再生
- よく使うフレーズ（「えらいね」「すごいね」等）は先にウォーム

成功条件:
- 音声返事がスムーズで、待ち時間が気にならない

## Phase 6: 習慣リマインド（風呂/保育園/就寝）
- 親がスケジュールを設定
- Cron → “そろそろ…”を自然に提示
- 押し付けない（拒否・先延ばしも会話として扱う）

成功条件:
- 子どもが嫌がらない（ここは超重要）

## Phase 7: 親ダッシュボード（言語獲得の面白さ）
- 語彙成長（初出/頻度）
- 言い回し（2語/3語連鎖）
- “概念”イベント抽出（反省/交渉/共感/予定/過去参照など）

成功条件:
- 親が「これおもしろい」と言える

## Phase 8: ハードニング（公開リポジトリとして）
- 認証（OIDC等）
- レート制限
- ログのマスキング
- バケットアクセス制御
- AI自動PRのガード強化
